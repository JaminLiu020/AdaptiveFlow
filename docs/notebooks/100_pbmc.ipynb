{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a0b6f9-b392-49fe-afb6-7b227bf5a73d",
   "metadata": {},
   "source": [
    "# Predicting donor-specific cytokine effects on PBMCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc061b8c-aaab-413f-8f20-a0f07c812dde",
   "metadata": {},
   "source": [
    "In this tutorial, we predict the donor-specific cytokine effect on PBMCs. The dataset comprises almost ten million cells from 12 different donors, whose samples were treated with 90 different cytokines. More information on the data can be found [here](https://www.parsebiosciences.com/datasets/10-million-human-pbmcs-in-a-single-experiment/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbffa5-7171-4c5e-8d21-c188af805502",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f0ce5-2981-4c5c-a8be-dc07de0b15ab",
   "metadata": {},
   "source": [
    "As the dataset is particularly large, we use [rapids-single-cell](https://rapids-singlecell.readthedocs.io/en/latest/index.html) which allows us to leverage GPU acceleration for preprocessing and downstream tasks. This notebook is executed on a A100 GPU with 80GB memory, as well as 500GB CPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07690e5c-523c-4a81-93c8-d57f89cabc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/dominik.klein/mambaforge/envs/cellflow/lib/python3.12/site-packages/optuna/study/_optimize.py:29: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from optuna import progress_bar as pbar_module\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", SettingWithCopyWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import jax\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import rapids_singlecell as rsc\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import cellflow\n",
    "from cellflow.model import CellFlow\n",
    "import cellflow.preprocessing as cfpp\n",
    "from cellflow.utils import match_linear\n",
    "from cellflow.plotting import plot_condition_embedding\n",
    "from cellflow.preprocessing import transfer_labels, compute_wknn, centered_pca, project_pca, reconstruct_pca\n",
    "from cellflow.metrics import compute_r_squared, compute_e_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05381d-1f6a-4907-9996-95ee2236495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"/lustre/groups/ml01/workspace/ot_perturbation/data/pbmc/adata_for_cellflow_datasets_with_embeddings.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557a706-93c6-41b1-9d79-482ae431b88b",
   "metadata": {},
   "source": [
    "Let's investigate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1582d-d193-4543-9d5c-cb63a8862a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21fc91-2d7c-4506-b941-259c1b9e36b6",
   "metadata": {},
   "source": [
    "We first create a column which saved the experimental condition, i.e. the combination of donor and treatment.\n",
    "Moreover, we require a boolean column which indicates if a cell is in control or perturbed state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8c286-0877-42cd-abff-1213e34759d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"condition\"] = adata.obs.apply(lambda x: x[\"donor\"] + \"_\" + x[\"cytokine\"], axis=1)\n",
    "adata.obs[\"is_control\"] = adata.obs.apply(lambda x: True if x[\"cytokine\"]==\"PBS\" else False, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504350ec-03f0-4c54-91cf-fd212f43bc71",
   "metadata": {},
   "source": [
    "We then normalize the data to a constant library size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a34cc0-5029-41d5-a426-074a28214625",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060054b-4a5d-4819-a1ce-c3fd7c1679c8",
   "metadata": {},
   "source": [
    "Similarly to the use case in the CellFlow manuscript, we aim to predict the response of donors to the IL-15 treatment. As we found performance to increase significantly as soon as the cytokine has been observed for one donor, we include the cytokine treatment for Donor 8, and predict the responses of the remaining donors.\n",
    "\n",
    "Therefore, we split our training data into training and test data. We note that the test data has to include the control populations (\"PBS\") of all donors whose response we want to predict. For this notebook, we restrict to predicting the response of donor 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d2187-f460-4c0c-a90d-28884ae98a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train = adata[(adata.obs[\"cytokine\"]!=\"IL-15\") | (adata.obs[\"donor\"]==\"Donor8\")].copy()\n",
    "adata_test = adata[((adata.obs[\"cytokine\"]==\"IL-15\") & (adata.obs[\"donor\"]!=\"Donor8\")) | (adata.obs[\"cytokine\"]==\"PBS\")].copy()\n",
    "adata_train.n_obs, adata_test.n_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a9a5b-5822-48ef-afc9-562363355d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train.n_obs, adata_test.n_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26e3b1e-7463-4daf-8c1b-77667b89717e",
   "metadata": {},
   "source": [
    "We now compute PCA on the training data, and then project the test data onto it. CellFlow implements these functions with GPU acceleration, which we can leverage using `method=\"rapids\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834503b-82cb-4e36-a94b-819bba2db08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfpp.centered_pca(adata_train, n_comps=100, method=\"rapids\", keep_centered_data=False)\n",
    "cfpp.project_pca(query_adata=adata_test, ref_adata=adata_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdfd08e-3793-44de-8a4f-456fa2d9bdcf",
   "metadata": {},
   "source": [
    "## Setting up the CellFlow model\n",
    "\n",
    "We are now ready to setup the {class}`~cellflow.model.CellFlow` model.\n",
    "\n",
    "Therefore, we first choose the flow matching solver. We select the solver `\"otfm\"`, which deterministically maps a cell to its perturbed equivalent. If we wanted to incorporate stochasticity on single-cell level, we would select `\"genot\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439abf26-eda6-4f0f-991d-dc850f5bdefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = CellFlow(adata_train, solver=\"otfm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1500afe-18b6-4d18-aa6a-91451548cca4",
   "metadata": {},
   "source": [
    "## Preparing {class}`~cellflow.model.CellFlow`'s data handling with {meth}`~cellflow.model.CellFlow.prepare_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10beb6-4401-4b68-8447-69591c393627",
   "metadata": {},
   "source": [
    "We now prepare the data. Therefore, we have to choose the sample representation, i.e. the space the (measure and generated) cells live in. We use {attr}`obsm['X_pca'] <anndata.AnnData.obsm>` as computed above. Moreover, we set the control key to {attr}`obs['is_control'] <anndata.AnnData.obs>`, as defined above.\n",
    "\n",
    "The `perturbation_covariates` indicates the external intervention, i.e. the cytokine treatment. We define a key (of arbitrary name) `\"cytokine_treatment\"` for this, and have the values be tuples with the perturbation and potential perturbation covariates. As we don't have a perturbation covariate (e.g. always the same dose), we only have one tuple, and as we don't observe combinations of treatments, the tuple has length 1. We use ESM2 embeddings for representing the cytokines, which we have precomputed already for the purpose of this notebook, saved in {attr}`uns['esm2_embeddings'] <anndata.AnnData.uns>`. Thus, we pass the information that `\"esm2_embeddings\"` stores embeddings of the {attr}`obs['cytokine'] <anndata.AnnData.obs>` treatments via `perturbation_covariate_reps`.\n",
    "\n",
    "The sample covariate describes the cellular context independent of the perturbation. In our case, these are donors, and given in the {attr}`obs['donor'] <anndata.AnnData.obs>` column. We use the mean of the control sample as donor representation, precomputed and saved in {attr}`uns['donor_embeddings'] <anndata.AnnData.uns>`. We thus pass this piece of information to {class}`~cellflow.model.CellFlow` via `sample_covariate_reps`. \n",
    "\n",
    "It remains to define `split_covariates`, according to which {class}`~cellflow.model.CellFlow` trains and predicts perturbations. In effect, `split_covariates` defines how to split the control distributions, and often coincides with `sample_covariates`. This ensure that we don't learn a mapping from the control distribution of donor A to a perturbed population of donor B, but only within the same donor. \n",
    "\n",
    "Finally, we can pass `max_combination_length` and `null_value`. These are relevant for combinations of treatments, which doesn't apply for this use case, as we don't want to predict combinationatorial effects of cytokines. In particular, `max_combination_length` is the maximum number of combinations of cytokines which we train on or we want to eventually predict for. The null value is the token representing no treatment, e.g. relevant when we have a treatment with fewer interventions than `max_combination_length`, see tutorials with combinatorial treatments as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1797f-4671-4a41-aeb3-978d91a8b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.prepare_data(\n",
    "    sample_rep = \"X_pca\",\n",
    "    control_key = \"is_control\",\n",
    "    perturbation_covariates = {\"cytokine_treatment\": (\"cytokine\",)},\n",
    "    perturbation_covariate_reps = {\"cytokine_treatment\": \"esm2_embeddings\"},\n",
    "    sample_covariates = [\"donor\"],\n",
    "    sample_covariate_reps = {\"donor\": \"donor_embeddings\"},\n",
    "    split_covariates = [\"donor\"],\n",
    "    max_combination_length = 1,\n",
    "    null_value = 0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc1515-30d3-4ee9-92bb-0c8299f94d21",
   "metadata": {},
   "source": [
    "We can now prepare the data for validation using {meth}`~cellflow.model.CellFlow.prepare_validation_data`. We can pass arbitrary splits, which we define with the `name` parameter. The corresponding {class}`adata <anndata.AnnData>` object has to contain the true value, such that during evaluation, we can compare the generated with the true cells.\n",
    "\n",
    "Note that inference takes relatively long due to solving a neural ODE, hence we might not want to evaluate on the full {class}`adata <anndata.AnnData>` objects, but only on a subset of conditions, the number of which we define using `n_conditions_on_log_iteration` and `n_conditions_on_train_end`. The number of cells we generate for each condition corresponds to the number of control cells, in our case to the number of control cells specific to each donor. As in this dataset the number of control cells is relatively large, we now first subsample the {class}`adata <anndata.AnnData>` object to accelerate inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10adef-6932-464c-b9b5-93b27a64f838",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas_train_subsampled = []\n",
    "for cond in adata_train.obs[\"condition\"].unique(): # as we have many conditions, this might take a few minutes\n",
    "    adatas_train_subsampled.append(sc.pp.subsample(adata_train[adata_train.obs[\"condition\"]==cond], n_obs=1000, copy=True))\n",
    "\n",
    "adata_train_for_validation = ad.concat(adatas_train_subsampled)\n",
    "\n",
    "adatas_test_subsampled = []\n",
    "for cond in adata_test.obs[\"condition\"].unique():\n",
    "    adatas_test_subsampled.append(sc.pp.subsample(adata_test[adata_test.obs[\"condition\"]==cond], n_obs=2000, copy=True))\n",
    "\n",
    "adata_test_for_validation = ad.concat(adatas_test_subsampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11baee21-52e5-4ca8-a828-9c580cb205aa",
   "metadata": {},
   "source": [
    "As we require the embeddings for evaluation as well, we need to copy {attr}`adata <anndata.AnnData.uns>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37f4b0-c165-4c45-b1f0-3b6c7755b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_train_for_validation.uns = adata_train.uns.copy()\n",
    "adata_test_for_validation.uns = adata_test.uns.copy()\n",
    "\n",
    "cf.prepare_validation_data(\n",
    "    adata_train_for_validation,\n",
    "    name=\"train\",\n",
    "    n_conditions_on_log_iteration=10,\n",
    "    n_conditions_on_train_end=10,\n",
    ")\n",
    "\n",
    "cf.prepare_validation_data(\n",
    "    adata_test_for_validation,\n",
    "    name=\"test\",\n",
    "    n_conditions_on_log_iteration=None,\n",
    "    n_conditions_on_train_end=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d7551-1a1a-4080-abfc-d8839724d7a2",
   "metadata": {},
   "source": [
    "## Preparing {class}`~cellflow.model.CellFlow`'s model architecture with {meth}`~cellflow.model.CellFlow.prepare_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f0ed52-4cd9-45c6-af62-3b230a49903c",
   "metadata": {},
   "source": [
    "We are now ready to specify the architecture of {class}`~cellflow.model.CellFlow`.\n",
    "\n",
    "We walk through the parameters one by one:\n",
    "\n",
    "- `encode_conditions` specifies whether we want to process the embeddings. We recommend to always set it to `True`.\n",
    "- `condition_mode` defines the structure of the learnt condition embedding space. We will use `deterministic` mode with     `regularization=0.0`, which means we learn point estimates of the condition embedding. If we added `regularization>0.0`, this would mean we impose some regularization with respect to the L2-norm of the embeddings. `condition_mode=\"stochastic\"` parameterizes the embeddings space as like a decoder-free variational auto-encoder, i.e. we set a normal isotropic prior on the embeddings, this allows to learn a stochastic mapping and evaluate the uncertainty of predictions on a distributional level (rather than on a single-cell level which can be done with {class}`~cellflow.solvers.GENOT`).\n",
    "- `regularization`, as mentioned above, is a tradeoff between the flow matching loss (which also implicitly learns the condition embeddings space), and the regularization of the mean and potentially the variance of the embedding space. Here, we learn point-wise estimates without any prior on the embeddings space, thus setting `regularization` to 0.0.\n",
    "- `pooling` defines how we aggregate combinations of conditions, which doesn't apply here. Putting `\"mean\"` thus has no effect, while `\"attention_token\"` or `\"attention_seed\"` would reduce to self-attention.\n",
    "- `pooling_kwargs` specifies further keyword arguments for {class}`~cellflow.networks.TokenAttentionPooling` if `pooling` is\n",
    "  `\"attention_token\"` or {class}`~cellflow.networks.SeedAttentionPooling` if `pooling` is `\"attention_seed\"`.\n",
    "- `layers_before_pool` specifies the layers processing the perturbation variables, i.e. perturbations, perturbation covariates, and sample covariates. It must be a dictionary with keys corresponding to the keys we used in {meth}`~cellflow.model.CellFlow.prepare_data`. In this case, this means that we have keys `\"cytokine_treatment\"` and `\"donor_embeddings\"`, with values specifying the architecture, e.g. the type of the module (`\"mlp\"` or `\"self_attention\"`) and layer specifications like number of layers, width, and dropout rate.\n",
    "- `layers_before_pool` specifies the architecture of the module after the pooling has been performed.\n",
    "- `condition_embedding_dim` is the dimension of the latent space of the condition encoder. We set it to 64.\n",
    "- `cond_output_dropout` is the dropout applied to the condition embedding, we recommend to set it relatively high, especially if the `condition_embedding_dim` is large.\n",
    "- `condition_encoder_kwargs` specify the architecture of the {class}`~cellflow.networks.ConditionEncoder`. Here, we don't apply any more specifications.\n",
    "- `pool_sample_covariates` defines whether the concatenation of the sample covariates should happen before or after pooling, in our case indicating whether it's part of the self-attention or only appended afterwards. \n",
    "- `time_freqs` thus (deterministically) embeds the time component before being processed by a feed-forward neural network. This choice is relatively independent of the data. \n",
    "- `time_encoder_dims` specifies the architecture how to process the time embedding needed for the neural ODE. Note that we pre-encode the time with a sinusoidal embedding of dimension `time_freqs`. This choice is relatively independent of the data. \n",
    "- `time_encoder_dropout` denotes the dropout applied to the layers processing the time component. This choice is relatively independent of the data. \n",
    "- `hidden_dims` specifies the layers processing the control cells. The choice depends on the dimensionality of the cell embedding.\n",
    "- `hidden_dropout` specifies the dropout in the layers defined by `hidden_dims`.\n",
    "- `decoder_dims` specifies the layers processing the embedding of the condition, the embedding of the cell, and the embedding of the time. It depends on the dimensionality of the cell representation, i.e. the higher-dimensional the cell representation, the higher `decoder_dims` should be chosen.\n",
    "- `decoder_dropout` sets the dropout rate of the layers processing `decoder_dims`.\n",
    "- `vf_act_fn` sets the activation function in the {class}`~cellflow.networks._velocity_field.ConditionalVelocityField` if not specified otherwise.\n",
    "- `vf_kwargs` provides further keyword arguments when the solver is not `\"otfm\"`, e.g. it provides keywords for {class}`~cellflow.networks._velocity_field.GENOTConditionalVelocityField`, which we don't require for this use case.\n",
    "- `flow` defines the reference vector field between pairs of samples which the {class}`~cellflow.networks._velocity_field.ConditionalVelocityField` is regressed against. Here, we use `{\"constant_noise\": 0.5}`, which internally applies {class}`~ott.neural.methods.flows.dynamics.ConstantNoiseFlow`. This means that the paths are augmented with random normal noise. Note that the magnitude should depend on the support / variance of the cell embedding. The higher the noise, the more the data is augmented, but the less the marginal distributions are fitted. To maintain convergence on the marginals, one can use `{\"bridge\"}\n",
    "- `match_fn` defines how to sample pairs batch-wise. If we have largely heterogeneous populations (e.g. whole embryos), we should choose a small entropic regularisation, while for homoegeneous cell populations like cell lines, a large entropic regularisation parameter is sufficient. Moreover, we can select the hyperparameters `tau_a` and `tau_b` determining the extent of unbalancedness in the learnt coupling, see e.g. moscot-tools.org for an in-depth discussion of optimal transport parameters.\n",
    "- `optimizer` should be used with gradient averaging to have parameter updates after having seen also multiple conditions, not only multiple cells. We found 20 to be a good value, but we recommend to perform a hyperparameter search. \n",
    "- `solver_kwargs` is primarily necessary for using a different {attr}`~cellflow.model.CellFlow.solver` than {class}`~cellflow.solvers.OTFlowMatching`, e.g. when using {class}`~cellflow.solvers.GENOT`. So this doesn't apply here.\n",
    "- `layer_norm_before_concatenation` determines whether to apply a linear layer before concatenating the condition embedding, the time embedding, and the cell embedding. It can be hyperparameterized over, but we generally found it to not significantly help.\n",
    "- `linear_projection_before_concatenation` applies linear layers to the embeddings of the condition, the time, and the cell. It can be hyperparameterized over, but we generally found it to not significantly help.\n",
    "- `seed` sets the seed for solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97455f-efd0-4b30-b6e5-70c82cf472fb",
   "metadata": {},
   "source": [
    "We start with defining the `layers_before_pool` and `layers_after_pool`. Note how similar the definitions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872cd261-882c-42d2-8f4f-f9eb81c7f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_before_pool = {\n",
    "    \"cytokine_treatment\": {\"layer_type\": \"mlp\", \"dims\": [1024, 1024], \"dropout_rate\": 0.5},\n",
    "    \"donor\": {\"layer_type\": \"mlp\", \"dims\": [256, 256], \"dropout_rate\": 0.0},\n",
    "}\n",
    "\n",
    "layers_after_pool = {\n",
    "    \"layer_type\": \"mlp\", \"dims\": [1024, 1024], \"dropout_rate\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4f20f-3b20-4196-b593-691df8a8159a",
   "metadata": {},
   "source": [
    "We now also explicitly define the `match_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8d037-1669-419a-80ba-d11d2b5768cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_fn = functools.partial(match_linear, epsilon=0.5, tau_a=1.0, tau_b=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0a272-6bab-4fc0-b316-b552be6641e9",
   "metadata": {},
   "source": [
    "Now we are ready to prepare the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e3d8ce-ce59-479a-9276-9c3432ddb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.prepare_model(\n",
    "    encode_conditions=True,\n",
    "    condition_mode=\"deterministic\",\n",
    "    regularization=0.0,\n",
    "    pooling=\"attention_token\",\n",
    "    pooling_kwargs={},\n",
    "    layers_before_pool=layers_before_pool,\n",
    "    layers_after_pool=layers_after_pool,\n",
    "    condition_embedding_dim=256,\n",
    "    cond_output_dropout=0.5,\n",
    "    condition_encoder_kwargs={},\n",
    "    pool_sample_covariates=True,\n",
    "    time_freqs=1024,\n",
    "    time_encoder_dims=[1024, 1024, 1024],\n",
    "    time_encoder_dropout=0.0,\n",
    "    hidden_dims=[2048, 2048, 2048],\n",
    "    hidden_dropout=0.0,\n",
    "    decoder_dims=[4096, 4096, 4096],\n",
    "    vf_act_fn=nn.silu,\n",
    "    vf_kwargs=None,\n",
    "    flow={\"constant_noise\": 0.5},\n",
    "    match_fn=match_fn,\n",
    "    optimizer=optax.MultiSteps(optax.adam(5e-5), 20),\n",
    "    solver_kwargs={},\n",
    "    layer_norm_before_concatenation=False,\n",
    "    linear_projection_before_concatenation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60ee3e-a325-4f80-95ff-f6e506552a20",
   "metadata": {},
   "source": [
    "## Computing and logging metrics during training \n",
    "\n",
    "For computing metrics during training, we provide callbacks. We divide callbacks into two categories: The first one performs computations, thus is an instance of {class}`~cellflow.training.ComputationCallback`; the second one are instances of {class}`~cellflow.training.LoggingCallback` and is used for logging. Users can either provide their own callbacks, or make use of existing ones, including {class}`~cellflow.training.Metrics` for computing metrics in the space which the cells are generated in, e.g. in PCA or VAE-space. For computing metrics in gene space, we can use {class}`~cellflow.training.PCADecodedMetrics` in case cells are PCA-embedded, or {class}`~cellflow.training.VAEDecodedMetrics` in case cells are embedding using {class}`~cellflow.external.CFJaxSCVI`. For computing metrics, we can provide user-defined ones, or metrics provided by CellFlow, which we will do below.\n",
    "\n",
    "For logging, we recommend using [Weights and Biases](https://wandb.ai), for which we provide a callback: {class}`~cellflow.training.WandbLogger`.\n",
    "\n",
    "As our cells live in PCA-space, we use the {class}`~cellflow.training.PCADecodedMetrics` callback, which takes as input also an {class}`adata <anndata.AnnData>` object which contains the PCs computed from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134dbb8-dea3-41ff-ba5b-cba755aeb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_callback = cellflow.training.Metrics(metrics=[\"r_squared\", \"mmd\", \"e_distance\"])\n",
    "decoded_metrics_callback = cellflow.training.PCADecodedMetrics(ref_adata=adata_train, metrics=[\"r_squared\"])\n",
    "wandb_callback = cellflow.training.WandbLogger(project=\"cellflow_tutorials\", out_dir=\"~\", config={\"name\": \"100m_pbmc\"})\n",
    "\n",
    "# we don't pass the wandb_callback as it requires a user-specific account, but recommend setting it up\n",
    "callbacks = [metrics_callback, decoded_metrics_callback]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fb8d8-909d-419f-85cb-18abf469c0c0",
   "metadata": {},
   "source": [
    "## Training CellFlow\n",
    "\n",
    "Finally, we are ready to train our model. It just remains to set the number of iterations (this depends on the number of conditions and cells, but should be between 50k and 1m), the batch size (the more heterogeneous the population, the larger), the callbacks which we have defined above, and the frequency of validation steps (note that inference takes relatively long, so once training behaviour is understood for a dataset, we can increase it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086daf9-c4ec-4327-a28f-55b86aaa287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.train(\n",
    "        num_iterations=500_000,\n",
    "        batch_size=1024,\n",
    "        callbacks=callbacks,\n",
    "        valid_freq=20_000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44171594-47e4-458d-8e29-34c3d5e2979f",
   "metadata": {},
   "source": [
    "We can now investigate some training statistics, stored by the {class}`~cellflow.training.CellFlowTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335fbd5-1b2c-4b56-a32a-651b83746c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.trainer.training_logs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0c4d1-f947-4cc4-8fc8-7a1257d24186",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_distances_train = cf.trainer.training_logs[\"train_e_distance_mean\"]\n",
    "e_distances_test = cf.trainer.training_logs[\"test_e_distance_mean\"]\n",
    "r_squared_test   = cf.trainer.training_logs[\"pca_decoded_test_r_squared_mean\"]\n",
    "\n",
    "iterations_train = np.arange(len(e_distances_train))\n",
    "iterations_test  = np.arange(len(e_distances_test))\n",
    "iterations_r2    = np.arange(len(r_squared_test))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].plot(iterations_train, e_distances_train, linestyle='-', color='blue', label='Energy distance training data')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Energy distance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(iterations_test, e_distances_test, linestyle='-', color='red', label='Energy distance test data')\n",
    "axes[1].set_xlabel('Validation iteration')\n",
    "axes[1].set_ylabel('Energy distance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].plot(iterations_r2, r_squared_test, linestyle='-', color='green', label='Mean R squared on test data')\n",
    "axes[2].set_xlabel('Validation iteration')\n",
    "axes[2].set_ylabel('R squared')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb6540-e54b-4746-afcf-49a8478c824f",
   "metadata": {},
   "source": [
    "## Investigating the learnt latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e33895-4d76-4113-97b1-8f46a3de9037",
   "metadata": {},
   "source": [
    "We can visualize the learnt latent space for any condition using {meth}`~CellFlow.get_condition_embedding`. Therefore, we have to provide a {class}`~pandas.DataFrame` with the same structure of {attr}`adata.obs <anndata.AnnData.obs>` (at least the columns which we used for {meth}`~cellflow.model.CellFlow.prepare_data`). Note that the embedding is independent of the cells, we thus don't need to pass the cellular representation. Moreover, {meth}`~cellflow.model.CellFlow.get_condition_embedding` returns both the learnt mean embedding and the logvariance. The latter is 0 when `condition_mode=\"stochastic\"`, hence we now only visualize the learnt mean. \n",
    "For now, let's use all conditions, but indicate whether they're seen during training or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83aa9b-7da8-4988-bdce-741f909edeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_data_train = adata_train[adata_train.obs[\"cytokine\"]!=\"PBS\"].obs.drop_duplicates(subset=[\"condition\"])\n",
    "covariate_data_test = adata_test[adata_test.obs[\"cytokine\"]!=\"PBS\"].obs.drop_duplicates(subset=[\"condition\"])\n",
    "\n",
    "df_embedding_train = cf.get_condition_embedding(covariate_data=covariate_data_train, condition_id_key=\"condition\", rep_dict=adata_train.uns)[0]\n",
    "df_embedding_test = cf.get_condition_embedding(covariate_data=covariate_data_test, condition_id_key=\"condition\", rep_dict=adata_train.uns)[0]\n",
    "\n",
    "df_embedding_train[\"seen_during_training\"] = True\n",
    "df_embedding_test[\"seen_during_training\"] = False\n",
    "df_condition_embedding = pd.concat((df_embedding_train, df_embedding_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078cfff2-f938-44f1-9630-491a4db408ca",
   "metadata": {},
   "source": [
    "We can now visualize the embedding, which is 256-dimensional, by calling {meth}`~cellflow.plotting.plot_condition_embedding`. We first visualize it according to whether it was seen during training or not. We choose a kernel PCA representation, but we recommend trying other dimensionaly reduction methods as well. We can see that the unseen conditions integrate well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5b57e-e994-4177-a59e-8e3bb46d577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_condition_embedding(df_condition_embedding, embedding=\"PCA\", hue=\"seen_during_training\", circle_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a17e0f-b6b5-4caf-a366-d2f829258b9c",
   "metadata": {},
   "source": [
    "Now we can colorize it according to donor and see that the first principal component captures the variability across donors ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be8a40-e111-4052-8f50-c9e203f6f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condition_embedding[\"condition\"] = df_condition_embedding.index\n",
    "df_condition_embedding[\"donor\"] = df_condition_embedding.apply(lambda x: x[\"condition\"].split(\"_\")[0], axis=1)\n",
    "fig = plot_condition_embedding(df_condition_embedding, embedding=\"PCA\", hue=\"donor\", circle_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7fa7c-1d50-418b-8f02-136f82f91072",
   "metadata": {},
   "source": [
    "... while the second principal component captures variance across cytokine treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac41aa-5018-437c-9462-a2adb0312c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cytos = adata.obs.drop_duplicates(subset=[\"cytokine\"])\n",
    "cyto_to_family = dict(df_cytos.set_index('cytokine')['cytokine_family'])\n",
    "    \n",
    "df_condition_embedding[\"cytokine\"] = df_condition_embedding.apply(lambda x: \"_\".join(x[\"condition\"].split(\"_\")[1:]), axis=1)\n",
    "df_condition_embedding[\"cytokine_family\"] = df_condition_embedding[\"cytokine\"].map(cyto_to_family)\n",
    "fig = plot_condition_embedding(df_condition_embedding, embedding=\"PCA\", hue=\"cytokine_family\", circle_size=50, legend=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619785af-e6e0-473d-bf9f-ee10984761fb",
   "metadata": {},
   "source": [
    "## Predicting with CellFlow\n",
    "\n",
    "Predictions with CellFlow require an {class}`adata <anndata.AnnData>` object with control cells; these can correspond to different sample covariates, i.e. to different donors. Moreover, we require `covariate_data` which are conditions which we would like to predict. In our case, we would like to predict the response to IL-15 from all donors which haven't been seen during training. For the sake of computational speed, we leverage our subsampled data containing 500 control cells per donor. As we only want to make counterfactual predictions for control cells, we have to filter the previously defined `adata_test_for_validation` to control cells only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5709fd8-4675-4805-ac90-25f2c5ead19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_ctrl_for_prediction = adata_test_for_validation[adata_test_for_validation.obs[\"is_control\"].to_numpy()]\n",
    "adata_ctrl_for_prediction.obs[\"condition\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b1830-9d75-40e3-a666-ec41851a58a8",
   "metadata": {},
   "source": [
    "The covariate data is supposed to contain the conditions which we would like to transform the source cells into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487fcb1-ca02-40c5-a2c0-a3563ac7fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_data_il15 = adata_test_for_validation[~adata_test_for_validation.obs[\"is_control\"].to_numpy()].obs.drop_duplicates(subset=[\"condition\"])\n",
    "covariate_data_il15.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1933f-61dc-4019-8363-bbc259539048",
   "metadata": {},
   "source": [
    "Now we can compute the predicted cellular states of IL-15 treatments for all donors which we haven't seen during training. Again, we specify where the cellular data is stored via `sample_rep`, and provide a `condition_id_key` which defines the keys of the returned dictionary containing the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28310d-825c-42b3-b50f-e8c3452e9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cf.predict(adata=adata_ctrl_for_prediction, sample_rep=\"X_pca\", condition_id_key=\"condition\", covariate_data=covariate_data_il15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936de826-30b1-4ff5-8770-ec2285e70712",
   "metadata": {},
   "source": [
    "We now build an {class}`adata <anndata.AnnData>` object to store the predictions in {attr}`adata.obsm <anndata.AnnData.obsm>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28534185-7ecc-4783-b30c-32adaafd9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_preds = []\n",
    "for cond, array in preds.items():\n",
    "    \n",
    "    obs_data = pd.DataFrame({\n",
    "        'condition': [cond] * array.shape[0]\n",
    "    })\n",
    "    adata_pred = ad.AnnData(X=np.empty((len(array),adata_train.n_vars)), obs=obs_data)\n",
    "    adata_pred.obsm[\"X_pca\"] = np.squeeze(array)\n",
    "    adata_preds.append(adata_pred)\n",
    "\n",
    "adata_preds = ad.concat(adata_preds)\n",
    "adata_preds.var_names = adata_train.var_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be230e-356b-4c3d-af12-e4e578d2023a",
   "metadata": {},
   "source": [
    "We now transfer labels using the 1NN classifier based on the training data. Therefore, we first have to compute a nearest neighbor graph, and then transfer the labels subsequently. Note that for the sake of computational speed, we here use subsampled training data `adata_train_for_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d33626-7dc6-4c1b-81ca-c054b2c8b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_wknn(ref_adata=adata_train_for_validation, query_adata=adata_preds, n_neighbors=1, ref_rep_key=\"X_pca\", query_rep_key=\"X_pca\")\n",
    "transfer_labels(query_adata=adata_preds, ref_adata=adata_train_for_validation, label_key=\"cell_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e8cec-faab-4555-8e60-fdcb42e24b1f",
   "metadata": {},
   "source": [
    "We now reconstruct the cells generated in PCA space to gene space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f5326-fb15-4d9f-97f8-ddfe37c656aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_pca(query_adata=adata_preds, use_rep=\"X_pca\", ref_adata=adata_train, layers_key_added=\"X_recon\")\n",
    "adata_preds.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6609bb3-fd18-4515-9cce-e056573f6acc",
   "metadata": {},
   "source": [
    "Now, we can plot our predictions together with transferred cell types, and compare it to the true cells, and a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1005a-5e99-4699-8bb8-75c8d0ebaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_preds.obs[\"donor\"] = adata_preds.obs.apply(lambda x: x[\"condition\"].split(\"_\")[0], axis=1)\n",
    "adata_preds.obs[\"cytokine\"] = adata_preds.obs.apply(lambda x: \"_\".join(x[\"condition\"].split(\"_\")[1:]), axis=1)\n",
    "adata_preds.obs[\"cell_type\"] = adata_preds.obs[\"cell_type_transfer\"]\n",
    "adata_preds.X = adata_preds.layers[\"X_recon\"]\n",
    "del adata.layers\n",
    "\n",
    "adata_preds.obs[\"mode\"] = \"generated\"\n",
    "adata_train_for_validation.obs[\"mode\"] = \"train\"\n",
    "adata_test_for_validation.obs[\"mode\"] = \"test\"\n",
    "\n",
    "adata_concatenated = ad.concat((adata_preds, adata_train_for_validation, adata_test_for_validation[~adata_test_for_validation.obs[\"is_control\"].to_numpy()]))\n",
    "\n",
    "rsc.pp.pca(adata_concatenated)\n",
    "rsc.pp.neighbors(adata_concatenated)\n",
    "rsc.tl.umap(adata_concatenated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcab7aa-df2f-4627-b7d0-004468e075c5",
   "metadata": {},
   "source": [
    "For a visual assessment, we compare true cells with generated cells on a UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738fb80-f483-4086-8346-c16f19b07bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_concatenated.obs[\"condition_mode\"] = adata_concatenated.obs.apply(lambda x: x[\"condition\"] + \"_\" + x[\"mode\"], axis=1)\n",
    "adata_concatenated.obs.reset_index(inplace=True)\n",
    "\n",
    "color_palette = {\"generated\": \"#B12F8C\", \"test\": \"#3F8AA6\", \"train\": \"#B6B6B6\"}\n",
    "order = list(adata_concatenated[adata_concatenated.obs[\"mode\"]==\"train\"].obs_names) + list(adata_concatenated[adata_concatenated.obs[\"mode\"]==\"test\"].obs_names) + list(adata_concatenated[adata_concatenated.obs[\"mode\"]==\"generated\"].obs_names)\n",
    "sc.pl.umap(adata_concatenated[order], color=[\"mode\"], palette=color_palette, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1ea20-6404-43ff-9196-54a44dbe9f59",
   "metadata": {},
   "source": [
    "We can also compare the true with the predicted population donor-specifically, together with a comparison to the control distribution, hence visualising both the donor effect and the perturbation effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63176b79-72cf-4d3a-95b3-1e526fedf68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_highlight(adata, donor_id, ax):\n",
    "    groups_to_visualize = [f\"{donor_id}_PBS_train\", f\"{donor_id}_IL-15_test\", f\"{donor_id}_IL-15_generated\"]\n",
    "    \n",
    "    # Define color palette\n",
    "    color_palette = {k: \"#B6B6B6\" for k in adata.obs[\"condition_mode\"].unique()}\n",
    "    color_palette.update({\n",
    "        f\"{donor_id}_IL-15_generated\": \"#B12F8C\",\n",
    "        f\"{donor_id}_IL-15_test\": \"#3F8AA6\",\n",
    "        f\"{donor_id}_PBS_train\": \"#000000\",\n",
    "    })\n",
    "    order = list(adata[~adata.obs[\"condition_mode\"].isin(groups_to_visualize)].obs_names) + \\\n",
    "            list(adata[adata.obs[\"condition_mode\"].isin(groups_to_visualize)].obs_names)\n",
    "    \n",
    "    sc.pl.umap(\n",
    "        adata[order],\n",
    "        color=\"condition_mode\",\n",
    "        groups=groups_to_visualize,\n",
    "        palette=color_palette,\n",
    "        size=10,\n",
    "        ax=ax,\n",
    "        show=False,\n",
    "        title=donor_id\n",
    "    )\n",
    "\n",
    "donors = [\"Donor1\", \"Donor9\", \"Donor7\", \"Donor10\"]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, donor in enumerate(donors):\n",
    "    plot_umap_highlight(adata_concatenated, donor, axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7b51c-9d90-4cf8-ae0f-027e8b1b09a1",
   "metadata": {},
   "source": [
    "Finally, we compute a few metrics. For evaluation of distributional metrics, we compute a lower-dimensional PCA space from the entire dataset (including the test data). For computational reasons, we subsample the entire dataset first, compute a PCA embedding, and subsequently project both the true and the generated cells onto it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677d97e-f593-4d64-aa00-05f75368864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_ref = sc.pp.subsample(adata, n_obs=100_000, copy=True)\n",
    "cfpp.centered_pca(adata_ref, n_comps=30, method=\"rapids\")\n",
    "adata_true = adata_test_for_validation[~adata_test_for_validation.obs[\"is_control\"].to_numpy()]\n",
    "cfpp.project_pca(query_adata=adata_preds, ref_adata=adata_ref)\n",
    "cfpp.project_pca(query_adata=adata_true, ref_adata=adata_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291a33e-962f-4d91-a787-8b69601b172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_target_encoded = {}\n",
    "test_data_target_decoded = {}\n",
    "test_data_target_encoded_predicted = {}\n",
    "test_data_target_decoded_predicted = {}\n",
    "\n",
    "for cond in adata_preds.obs[\"condition\"].unique():\n",
    "    test_data_target_encoded[cond] = adata_true[adata_true.obs[\"condition\"] == cond].obsm[\"X_pca\"]\n",
    "    test_data_target_decoded[cond] = adata_true[adata_true.obs[\"condition\"] == cond].X.toarray()\n",
    "    test_data_target_encoded_predicted[cond] = adata_preds[adata_preds.obs[\"condition\"] == cond].obsm[\"X_pca\"]\n",
    "    test_data_target_decoded_predicted[cond] = adata_preds[adata_preds.obs[\"condition\"] == cond].X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cab2b8-8803-43e5-ae3a-eb7c18962034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "e_distances = jax.tree_util.tree_map(\n",
    "    compute_e_distance, test_data_target_encoded, test_data_target_encoded_predicted\n",
    ")\n",
    "\n",
    "r_squared = jax.tree_util.tree_map(\n",
    "    compute_r_squared, test_data_target_decoded, test_data_target_decoded_predicted\n",
    ")\n",
    "\n",
    "df_e_distance = pd.DataFrame.from_dict(e_distances, orient=\"index\", columns=[\"energy_distance\"])\n",
    "df_r_squared = pd.DataFrame.from_dict(r_squared, orient=\"index\", columns=[\"r_squared\"])\n",
    "df_metrics = pd.merge(df_e_distance, df_r_squared, left_index=True, right_index=True)\n",
    "df_metrics[\"condition\"] = df_metrics.index\n",
    "df_metrics[\"donor\"] = df_metrics.apply(lambda x: x[\"condition\"].split(\"_\")[0], axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "sns.barplot(\n",
    "    data=df_metrics,\n",
    "    x=\"donor\",\n",
    "    y=\"energy_distance\",\n",
    "    edgecolor=\"black\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Energy Distance by Donor\", fontsize=14)\n",
    "axes[0].set_xlabel(\"Donor\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Energy Distance\", fontsize=12)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: R squared\n",
    "sns.barplot(\n",
    "    data=df_metrics,\n",
    "    x=\"donor\",\n",
    "    y=\"r_squared\",\n",
    "    edgecolor=\"black\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"R squared in Gene Expression Space\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Donor\", fontsize=12)\n",
    "axes[1].set_ylabel(\"R squared\", fontsize=12)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellflow",
   "language": "python",
   "name": "cellflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
